# URL сервера vLLM (OpenAI-совместимый API). vLLM запускается отдельно (например в tmux на хосте).
# Важно: если qwen-bbox-ocr в Docker, а vLLM на том же хосте — из контейнера localhost = сам контейнер, запросы до vLLM не дойдут.
# Укажите адрес хоста:
#   Linux (Docker):  VLLM_BASE_URL=http://172.17.0.1:8000/v1
#   Windows/Mac:    VLLM_BASE_URL=http://host.docker.internal:8000/v1
# Если оба на хосте (без Docker):  VLLM_BASE_URL=http://localhost:8000/v1
VLLM_BASE_URL=http://172.17.0.1:8000/v1

# Имя модели, как зарегистрировано на vLLM (должно соответствовать загруженной модели)
VLLM_MODEL=Qwen/Qwen3-VL-235B-A22B

# Опционально: API key (vLLM часто без ключа — можно оставить пустым)
# VLLM_API_KEY=

# Таймаут запроса к vLLM (секунды) и макс. токенов ответа (см. README — лимит токенов)
VLLM_TIMEOUT_SECONDS=300
VLLM_MAX_TOKENS=2048

# DPI при конвертации PDF в изображения. Меньше DPI — меньше картинка и входных токенов, больше остаётся на ответ.
# PDF_DPI=100
PDF_DPI=150
